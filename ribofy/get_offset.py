"""
get_offset determines the optimal p-site offset for each read-length on the top 10 most abundant ORFs in the bam-file


usage:
python get_offset.py --bam <bam-file> --orfs <ribofy orfs-file> --output <output-file>

By default, get_offset analyses reads between 25 and 35 nt, 
but this is customizable with the --min_read_length and --max_read_length options

And change number of ORFs used in offset calculation by the --norfs option

"""

import pysam
import pandas as pd
import numpy as np
from collections import Counter
from .argparse2 import argparse2
from .get_phasing import get_phasing_stats, get_phasing_matrix
from .bam_utils import get_tid_info


# def agg_percentile(n, postfix = ""):
#     def percentile_(x):
#         return x.quantile(n)
#     percentile_.__name__ = ('percentile_' + postfix).strip ("_")
#     return percentile_

# def agg_table(ret = "key"): #key/value/pct
#     def table_(x):
#         ctr = Counter(x).most_common()
#         if ret == "key":            
#             return ([k for (k,v) in ctr][0])
#         elif ret == "value":
#             return ([v for (k,v) in ctr][0])
#         elif ret == "pct":
#             return ([v/len(x) for (k,v) in ctr][0])


#     table_.__name__ = f"table_{ret}"
#     return table_

def agg_f (x, p_methods, percentile):
    """Aggregate function to summarize the offset data
    """
    
    d = {}

    for s in p_methods:
        d["p_"+s] = x[s].quantile(percentile) 

    ctr = Counter(x['offset']).most_common()
    
    d['offset_key'] = [k for (k,v) in ctr][0]
    d['offset_value'] = [v for (k,v) in ctr][0]
    d['offset_pct'] = [v/len(x) for (k,v) in ctr][0]

    for c in ['frame0', 'frame1', 'frame2']:
        d[c] = np.sum (x[c])
   
    return (pd.Series(d, index=[i for i in d])) #, index=['a_sum', 'a_max', 'b_mean', 'c_d_prodsum'])


def get_offset (bamfiles, orfs, output, norfs=20, min_read_length=25, max_read_length=35, percentile = .9, p_methods = ['glm'], full_output = ""):
    """Main function: based on bamfiles and pre-established ORFs, the most likely offsets from 25-35 read lengths are infered based on the top20 expressed genes. 
    bamfiles: list of bamfiles to analyse
    orfs: Pre-defined ORFs (generated by ribofy orfs)
    output: output file
    norfs: Number of orfs to use in the offset analysis (default: 20)
    min/max_read_length: the interval of read_lengths analysis (default: 25-35)
    percentile: From the norfs analysed use the percentile best to establish offset (this ensures that single outliers are not invalidating the offset results
    p_methods: statistics used in evaluating offsets   
    """

    print ("### getting p-site offsets ###")
    
    pd_orfs = pd.read_csv (orfs, sep="\t") if isinstance (orfs, str) else orfs

    pd_annot = pd_orfs[(pd_orfs.orf_type == "annotated") & (pd_orfs.orf_length >= 500)] \
        .groupby ("orf_group") \
        .head (1) 

    pd_output = pd.DataFrame ()
    pd_full = pd.DataFrame ()
    
    for bamfile in bamfiles:

        # get transcripts with most counts
        print (f"infering offsets for bam ({bamfile})...")

        # load bam
        bam = pysam.Samfile (bamfile)
        
        dtid2count, dtid2ref = get_tid_info (bamfile)
        
        # add read counts to dataframe
        pd_annot['total_reads'] = pd_annot['tid'].transform (lambda x: dtid2count[x] if x in dtid2count else 0)
        
        pd_annot = pd_annot.sort_values ('total_reads', ascending=False)

        # initialize count_offsets
        length_range = range (min_read_length, max_read_length+1) 
        
        count_offsets = {}

        for x in length_range:
            count_offsets[x] = [0,0,0]

        off_conv = {0:0, 1:2, 2:1}

        offset_stats = []

        for i, row in pd_annot.head (norfs).iterrows ():

            tid, start, end = dtid2ref[row['tid']], int(row['start']), int(row['stop'])+3
            orf_id = row['orf_id']

            dcds = {}
            for lr in length_range:
                dcds[lr] = [0] * (end-start)
                    
            for read in bam.fetch (tid, start, end):

                if read.is_reverse:
                    continue

                init_offset_pos = read.pos + 12

                read_length = read.infer_read_length () 

                if read_length < min_read_length or read_length > max_read_length:
                    continue

                if init_offset_pos >= start and init_offset_pos < end: 

                    init_rel_pos = init_offset_pos - start            
                    offset = off_conv[init_rel_pos % 3]
                    
                    rel_pos = (12 + offset + read.pos) - start

                    if rel_pos % 3 != 0:
                        print ("something wrong with offset")
        
                    count_offsets[read_length][offset] += 1
                    
                    if init_rel_pos >= 0 and init_rel_pos < len (dcds[read_length]): 
                        dcds[read_length][init_rel_pos] += 1

            
            # check phasing for each read length individually
            for lr in length_range:
                
                mat = get_phasing_matrix (dcds[lr])

                frame_sort = np.argsort(mat.sum(axis=0))[::-1]

                # set the frame with most count as onframe (index 0)
                mat = mat[:,frame_sort]
                
                output_stats = get_phasing_stats (mat, p_methods)

                output_stats['read_length'] = lr
                output_stats['tid'] = tid
                output_stats['orf_id'] = orf_id
                output_stats['offset'] = 12 + off_conv[frame_sort[0]] # best offset
                output_stats['frame0'] = np.sum (mat[:,0])  # sum of reads with onframe psites
                output_stats['frame1'] = np.sum (mat[:,1])
                output_stats['frame2'] = np.sum (mat[:,2])  

                offset_stats.append(output_stats)

        pd_stats = pd.DataFrame (offset_stats)

        if full_output != "":
            pd_stats['bam'] = bamfile
            pd_full = pd.concat ([pd_full, pd_stats])
            
        # for each read-length aggregate the results for the analysed transcripts        
        pd_stats = pd_stats \
            .dropna() \
            .groupby ('read_length') \
            .apply (agg_f, p_methods=p_methods, percentile=percentile)
        
        pd_stats['bam'] = bamfile

        pd_output = pd.concat ([pd_output, pd_stats])

    print ("extracted offsets:")
    print (pd_output[['bam', 'offset_key', 'offset_pct']])

    # save to output
    pd_output.to_csv (output, sep="\t")

    if full_output != "":
        pd_full.to_csv (full_output, sep="\t")

    pd_output['read_length'] =  pd_output.index

    return (pd_output)

    
   

def ribofy_offset ():

    parser = argparse2 (
        description="",
        usage="",
        help=""
    )

    parser.add_argument('detect', nargs='?', help='') # dummy argument
    parser._action_groups.pop()

    parser.add_argument("--bam",   dest='bam', nargs="+", required=True, help="bam file - sorted and indexed")
    parser.add_argument("--orfs",  dest='orfs', required=True, help="orfs - generated by get_ORFs.py")
    parser.add_argument("--output", dest='output', default = "ribofy_offsets.txt", help="output")
    
    #optional
    parser.add_argument('--norfs', dest='norfs', default = 20, type = int, help="number of distinct orfs to build offsets")
    parser.add_argument('--min_read_length', dest='min_read_length', default = 25, type = int, help="minimum read length used in analysis")
    parser.add_argument('--max_read_length', dest='max_read_length', default = 35, type = int, help="maximum read length used in analysis")
    parser.add_argument("--percentile", dest='percentile', default = 0.9, help="Percentile of consistent offset-determinants")
    parser.add_argument("--p_methods", dest='p_methods', nargs="*", default = ["binom", "wilcox", "glm"], help="Statistics: possibilities: binom, wilcox, glm, and taper")

    parser.add_argument("--full_output", dest='full_output', default = "", help="output")
    
    args = parser.parse_args()

    get_offset (args.bam, args.orfs, 
                output=args.output, 
                norfs=args.norfs, 
                min_read_length = args.min_read_length,
                max_read_length = args.max_read_length,
                percentile=args.percentile,
                p_methods=args.p_methods,
                full_output= args.full_output)


if __name__ == "__main__":
        
    ribofy_offset ()