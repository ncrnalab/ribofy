"""
get_offset determines the optimal p-site offset for each read-length on the top 10 most abundant ORFs in the bam-file


usage:
python get_offset.py --bam <bam-file> --orfs <ribofy orfs-file> --output <output-file>

By default, get_offset analyses reads between 25 and 35 nt, 
but this is customizable with the --min_read_length and --max_read_length options

And change number of ORFs used in offset calculation by the --norfs option

"""


import sys
import argparse
import pysam
import pandas as pd

from .utils import get_tid_info


def get_offset (bamfile, orfs, output, norfs=10, min_read_length=25, max_read_length=35):

    print ("### get_offset ###")

    print ("loading orfs...")

    pd_orfs = pd.read_csv (orfs, sep="\t")
    pd_orfs = pd_orfs[(pd_orfs.orf_type == "annotated")] \
        .groupby ("orf_group") \
        .head (1) 

    # get transcripts with most counts
    print ("get transcript counts from bam...")

    # load bam
    bam = pysam.Samfile (bamfile)
    
    dtid2count, dtid2ref = get_tid_info (bamfile)

    
    # add read counts to dataframe
    pd_orfs['total_reads'] = pd_orfs['tid'].transform (lambda x: dtid2count[x])
    pd_orfs = pd_orfs.sort_values ('total_reads', ascending=False)

    print ("using the following transcripts..:")
    print (pd_orfs[['tid', 'total_reads']].head (norfs))


    # initialize count_offsets
    length_range = range (min_read_length, max_read_length+1) 
    count_offsets = {}

    for x in length_range:
        count_offsets[x] = [0,0,0]

    off_conv = {0:0, 1:2, 2:1}

    for i, row in pd_orfs.head (norfs).iterrows ():

        tid, start, end = dtid2ref[row['tid']], int(row['start']), int(row['stop'])
                
        for read in bam.fetch (tid, start, end):

            #cds = [0] * (end-start+1)
            
            init_offset_pos = read.pos + 12

            read_length = read.infer_read_length () 

            if read_length < min_read_length or read_length > max_read_length:
                continue

            if init_offset_pos >= start-3 and init_offset_pos < end+3: 

                init_rel_pos = init_offset_pos - start            
                offset = off_conv[init_rel_pos % 3]
                
                rel_pos = (12 + offset + read.pos) - start

                if rel_pos % 3 != 0:
                    print ("something wrong with offset")
       
                count_offsets[read_length][offset] += 1
       
    # compile dataframe
    pd_offsets = pd.DataFrame ({        
        'read_length' : list (length_range),
        'offset' : [str(12+count_offsets[x].index (max(count_offsets[x]))) for x in length_range],
        'reads12' : [count_offsets[x][0] for x in length_range],
        'reads13' : [count_offsets[x][1] for x in length_range],
        'reads14' : [count_offsets[x][2] for x in length_range],
        'bam' : bamfile
    })

    pd_offsets.to_csv (output, sep="\t")

    print ("extracted offsets:")
    print (pd_offsets[['read_length', 'offset']])
    print ("done")

if __name__ == "__main__":
        
    parser = argparse.ArgumentParser(description='get ORFs from GTF')
    parser.add_argument("--bam",   dest='bam', required=True, help="bam file - sorted and indexed")
    parser.add_argument("--orfs",  dest='orfs', required=True, help="orfs - generated by get_ORFs.py")
    parser.add_argument("--output", dest='output', default = "ribofy_offsets.txt", help="output")
    
    #optional
    parser.add_argument('--norfs', dest='norfs', default = 10, type = int, help="number of distinct orfs to build offsets")
    parser.add_argument('--min_read_length', dest='min_read_length', default = 25, type = int, help="minimum read length used in analysis")
    parser.add_argument('--max_read_length', dest='max_read_length', default = 35, type = int, help="maximum read length used in analysis")
    
    args = parser.parse_args()

    get_offset (args.bam, args.orfs, args.output, 
                norfs=args.norfs, 
                min_read_length = args.min_read_length,
                max_read_length = args.max_read_length)


